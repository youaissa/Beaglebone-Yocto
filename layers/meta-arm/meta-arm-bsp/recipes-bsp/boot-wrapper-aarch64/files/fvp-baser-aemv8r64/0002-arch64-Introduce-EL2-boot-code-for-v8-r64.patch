From 81fcc5cc80c9c3c812d92000b9116f6a02ff7e6c Mon Sep 17 00:00:00 2001
Message-Id: <81fcc5cc80c9c3c812d92000b9116f6a02ff7e6c.1616744115.git.diego.sueiro@arm.com>
In-Reply-To: <7b8c821c22929cd2d3532f937672fcf05dc7d5d0.1616744115.git.diego.sueiro@arm.com>
References: <7b8c821c22929cd2d3532f937672fcf05dc7d5d0.1616744115.git.diego.sueiro@arm.com>
From: Jaxson Han <jaxson.han@arm.com>
Date: Thu, 25 Mar 2021 12:47:02 +0800
Subject: [PATCH 2/2] arch64: Introduce EL2 boot code for v8-r64

The v8-r64 boots from EL2 mode. In order to boot linux, EL2 boot mode
is needed. Because there is no MMU supported for v8-r64 under EL2 mode,
bootwrapper needs to switch to EL1 mode when jumpping to the kernel.

Some register in gic-v3.h need to be auto-detected.

Issue-ID: SCM-2221
Signed-off-by: Jaxson Han <jaxson.han@arm.com>
Change-Id: I52ca3f045f1ab50f32945420144752f396d95193

Upstream-Status: Pending
Signed-off-by: Diego Sueiro <diego.sueiro@arm.com>
---
 arch/aarch64/boot.S               | 76 +++++++++++++++++++++++++++----
 arch/aarch64/include/asm/cpu.h    |  3 ++
 arch/aarch64/include/asm/gic-v3.h | 23 ++++++++--
 arch/aarch64/psci.S               | 13 +++---
 arch/aarch64/spin.S               |  8 ++--
 arch/aarch64/utils.S              |  8 ++++
 6 files changed, 110 insertions(+), 21 deletions(-)

diff --git a/arch/aarch64/boot.S b/arch/aarch64/boot.S
index e47cf59..5c3eb73 100644
--- a/arch/aarch64/boot.S
+++ b/arch/aarch64/boot.S
@@ -22,20 +22,30 @@ _start:
 	bl	setup_stack
 
 	/*
-	 * EL3 initialisation
+	 * Boot sequence
+	 * If EL3, goto EL3 initialisation
+	 * If EL2 && id_aa64mmfr0_el1.MSA == 0xf, do Armv8r initialisation
+	 * Else no initialisation sequence
 	 */
 	mrs	x0, CurrentEL
 	cmp	x0, #CURRENTEL_EL3
-	b.eq	1f
+	beq	el3_init
+	cmp	x0, #CURRENTEL_EL2
+	beq	el2_init
 
+no_el_max:
 	mov	w0, #1
 	ldr	x1, =flag_no_el3
 	str	w0, [x1]
 
 	bl	setup_stack
-	b	start_no_el3
+	b	start_no_el_max
 
-1:	mov	x0, #0x30			// RES1
+	/*
+	 * EL3 initialisation
+	 */
+el3_init:
+	mov	x0, #0x30			// RES1
 	orr	x0, x0, #(1 << 0)		// Non-secure EL1
 	orr	x0, x0, #(1 << 8)		// HVC enable
 
@@ -93,14 +103,54 @@ _start:
 	mov	x0, #ZCR_EL3_LEN_MASK		// SVE: Enable full vector len
 	msr	ZCR_EL3, x0			// for EL2.
 
-1:
+	mov	w0, #SPSR_KERNEL
+	ldr	x1, =spsr_to_elx
+	str	w0, [x1]
+	b	el_max_init
+
+	/*
+	 * EL2 Armv8r initialisation
+	 */
+el2_init:
+	/* Detect Armv8r */
+	mrs	x1, id_aa64mmfr0_el1
+	ubfx	x1, x1, #48, #4			// MSA
+	cmp	x1, 0xf				// 0xf means Armv8r
+	bne	no_el_max
+
+	mrs	x0, midr_el1
+	msr	vpidr_el2, x0
+
+	mrs	x0, mpidr_el1
+	msr	vmpidr_el2, x0
+
+	mov	x0, #(1 << 31)			// VTCR_MSA: VMSAv8-64 support
+	msr	vtcr_el2, x0
+
+	/* Enable pointer authentication if present */
+	mrs	x1, id_aa64isar1_el1
+	ldr	x2, =(((0xff) << 24) | (0xff << 4))
+	and	x1, x1, x2
+	cbz	x1, 1f
+
+	mrs	x0, hcr_el2
+	orr	x0, x0, #(1 << 40)		// AP key enable
+	orr	x0, x0, #(1 << 41)		// AP insn enable
+	msr	hcr_el2, x0
+
+1:	isb
+	mov	w0, #SPSR_KERNEL_EL1
+	ldr	x1, =spsr_to_elx
+	str	w0, [x1]
+	b	el_max_init
+
+el_max_init:
 	ldr	x0, =CNTFRQ
 	msr	cntfrq_el0, x0
 
 	bl	gic_secure_init
 
-	b	start_el3
-
+	b	start_el_max
 err_invalid_id:
 	b	.
 
@@ -137,7 +187,7 @@ jump_kernel:
 	b.eq	1f
 	br	x19			// No EL3
 
-1:	mov	x4, #SPSR_KERNEL
+1:	ldr	w4, spsr_to_elx
 
 	/*
 	 * If bit 0 of the kernel address is set, we're entering in AArch32
@@ -145,13 +195,23 @@ jump_kernel:
 	 */
 	bfi	x4, x19, #5, #1
 
+	mrs	x18, CurrentEL
+	cmp	x18, #CURRENTEL_EL2
+	b.eq	1f
+
 	msr	elr_el3, x19
 	msr	spsr_el3, x4
 	eret
 
+1:	msr	elr_el2, x19
+	msr	spsr_el2, x4
+	eret
+
 	.ltorg
 
 	.data
 	.align 3
 flag_no_el3:
 	.long 0
+spsr_to_elx:
+	.long 0
diff --git a/arch/aarch64/include/asm/cpu.h b/arch/aarch64/include/asm/cpu.h
index ccb5397..2b3a0a4 100644
--- a/arch/aarch64/include/asm/cpu.h
+++ b/arch/aarch64/include/asm/cpu.h
@@ -11,6 +11,7 @@
 
 #define MPIDR_ID_BITS		0xff00ffffff
 
+#define CURRENTEL_EL2		(2 << 2)
 #define CURRENTEL_EL3		(3 << 2)
 
 /*
@@ -24,6 +25,7 @@
 #define SPSR_I			(1 << 7)	/* IRQ masked */
 #define SPSR_F			(1 << 6)	/* FIQ masked */
 #define SPSR_T			(1 << 5)	/* Thumb */
+#define SPSR_EL1H		(5 << 0)	/* EL1 Handler mode */
 #define SPSR_EL2H		(9 << 0)	/* EL2 Handler mode */
 #define SPSR_HYP		(0x1a << 0)	/* M[3:0] = hyp, M[4] = AArch32 */
 
@@ -42,6 +44,7 @@
 #else
 #define SCTLR_EL1_RESET		SCTLR_EL1_RES1
 #define SPSR_KERNEL		(SPSR_A | SPSR_D | SPSR_I | SPSR_F | SPSR_EL2H)
+#define SPSR_KERNEL_EL1		(SPSR_A | SPSR_D | SPSR_I | SPSR_F | SPSR_EL1H)
 #endif
 
 #ifndef __ASSEMBLY__
diff --git a/arch/aarch64/include/asm/gic-v3.h b/arch/aarch64/include/asm/gic-v3.h
index e743c02..f8ddb27 100644
--- a/arch/aarch64/include/asm/gic-v3.h
+++ b/arch/aarch64/include/asm/gic-v3.h
@@ -15,21 +15,38 @@
 #define ICC_CTLR_EL3	"S3_6_C12_C12_4"
 #define ICC_PMR_EL1	"S3_0_C4_C6_0"
 
+static inline uint32_t current_el(void)
+{
+	uint32_t val;
+
+	asm volatile ("mrs %0, CurrentEL" : "=r" (val));
+	return val;
+}
+
 static inline uint32_t gic_read_icc_sre(void)
 {
 	uint32_t val;
-	asm volatile ("mrs %0, " ICC_SRE_EL3 : "=r" (val));
+
+	if(current_el() == CURRENTEL_EL3)
+		asm volatile ("mrs %0, " ICC_SRE_EL3 : "=r" (val));
+	else
+		asm volatile ("mrs %0, " ICC_SRE_EL2 : "=r" (val));
+
 	return val;
 }
 
 static inline void gic_write_icc_sre(uint32_t val)
 {
-	asm volatile ("msr " ICC_SRE_EL3 ", %0" : : "r" (val));
+	if(current_el() == CURRENTEL_EL3)
+		asm volatile ("msr " ICC_SRE_EL3 ", %0" : : "r" (val));
+	else
+		asm volatile ("msr " ICC_SRE_EL2 ", %0" : : "r" (val));
 }
 
 static inline void gic_write_icc_ctlr(uint32_t val)
 {
-	asm volatile ("msr " ICC_CTLR_EL3 ", %0" : : "r" (val));
+	if(current_el() == CURRENTEL_EL3)
+		asm volatile ("msr " ICC_CTLR_EL3 ", %0" : : "r" (val));
 }
 
 #endif
diff --git a/arch/aarch64/psci.S b/arch/aarch64/psci.S
index 01ebe7d..0681d5e 100644
--- a/arch/aarch64/psci.S
+++ b/arch/aarch64/psci.S
@@ -45,8 +45,8 @@ vector:
 
 	.text
 
-	.globl start_no_el3
-	.globl start_el3
+	.globl start_no_el_max
+	.globl start_el_max
 
 err_exception:
 	b err_exception
@@ -101,7 +101,7 @@ smc_exit:
 	eret
 
 
-start_el3:
+start_el_max:
 	ldr	x0, =vector
 	bl	setup_vector
 
@@ -111,10 +111,11 @@ start_el3:
 	b	psci_first_spin
 
 /*
- * This PSCI implementation requires EL3. Without EL3 we'll only boot the
- * primary cpu, all others will be trapped in an infinite loop.
+ * This PSCI implementation requires EL3 or AArch64-R EL2. Without EL max
+ * we'll only boot the primary cpu, all others will be trapped in an infinite
+ * loop.
  */
-start_no_el3:
+start_no_el_max:
 	cpuid	x0, x1
 	bl	find_logical_id
 	cbz	x0, psci_first_spin
diff --git a/arch/aarch64/spin.S b/arch/aarch64/spin.S
index 72603cf..fa1d657 100644
--- a/arch/aarch64/spin.S
+++ b/arch/aarch64/spin.S
@@ -11,11 +11,11 @@
 
 	.text
 
-	.globl start_no_el3
-	.globl start_el3
+	.globl start_no_el_max
+	.globl start_el_max
 
-start_el3:
-start_no_el3:
+start_el_max:
+start_no_el_max:
 	cpuid	x0, x1
 	bl	find_logical_id
 
diff --git a/arch/aarch64/utils.S b/arch/aarch64/utils.S
index ae22ea7..2a63fa7 100644
--- a/arch/aarch64/utils.S
+++ b/arch/aarch64/utils.S
@@ -41,6 +41,14 @@ find_logical_id:
  * x0: vector address
  */
 setup_vector:
+	mrs	x1, CurrentEL
+	cmp	x1, #CURRENTEL_EL2
+	b.eq	1f
+
 	msr	VBAR_EL3, x0
 	isb
 	ret
+
+1:	msr	VBAR_EL2, x0
+	isb
+	ret
-- 
2.17.1

